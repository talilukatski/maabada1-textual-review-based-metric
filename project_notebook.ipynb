{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04229252-427a-4364-8c19-85eda47edca7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a21ab0af-8722-477c-9206-68706e06477b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d9c4eb8-842d-44eb-b213-8652fd8f6940",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PySpark SQL\n",
    "from pyspark.sql import functions as F, Window\n",
    "from pyspark.sql.functions import col, count, when, isnan, trim, broadcast\n",
    "from pyspark.sql.types import StringType, BooleanType\n",
    "\n",
    "# PySpark ML\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, Word2Vec\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Spark NLP\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from sparknlp.annotator import Tokenizer, BertForSequenceClassification\n",
    "\n",
    "# Language detection\n",
    "from langdetect import detect, DetectorFactory\n",
    "\n",
    "# Utilities\n",
    "from functools import reduce\n",
    "import re\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf769843-1a1a-4a6e-ba8f-a13e2244a9cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Train Data:\n",
    "- sample scraped booking + expedia\n",
    "\n",
    "Test Data:\n",
    "- scraped booking + expedia\n",
    "- sample original booking\n",
    "- sample original airbnb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e497cb4-9e2e-451b-bbc2-c676e9859103",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### scraped booking dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67b2f470-4023-41e2-8655-0e6bf839daf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account = \"lab94290\" \n",
    "container = \"submissions\"\n",
    "group = \"itay_asaf_antal\"\n",
    "sas_token = \"...\" # change to your sas token\n",
    "sas_token = sas_token.lstrip('?')\n",
    "\n",
    "# 2) tell ABFS to use SAS for the account and give the fixed token provider\n",
    "acct = storage_account\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{acct}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{acct}.dfs.core.windows.net\",\n",
    "               \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{acct}.dfs.core.windows.net\", sas_token)\n",
    "\n",
    "output_path = f\"abfss://{container}@{acct}.dfs.core.windows.net/{group}/scraped_booking.csv\"\n",
    "\n",
    "scraped_booking = (spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .option(\"escape\", '\"')\n",
    "    .option(\"quote\", '\"')\n",
    "    .load(output_path)\n",
    ")\n",
    "\n",
    "print(\"number of rows is\", scraped_booking.count())\n",
    "display(scraped_booking.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91d4ee1c-329d-4c4b-ae73-6139a3a1b6d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scraped_booking = scraped_booking.withColumn(\n",
    "    \"hotel_id\",\n",
    "    F.concat_ws(\", \", F.col(\"HotelName\"), F.col(\"City\"), F.col(\"Country\"))\n",
    ").withColumn(\"hotel_id\", F.lower(F.col(\"hotel_id\")))\n",
    "\n",
    "scraped_booking = scraped_booking.select(\"hotel_id\",\n",
    "                F.col(\"Review\").alias(\"text_review\"),\n",
    "                F.col(\"Rating\").alias(\"label\")\n",
    "                )\n",
    "display(scraped_booking.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5accbdcd-62c5-4d1f-a814-a48ec84d22e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### scraped expedia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d22a287-00f2-4a9b-b5fe-b1c3774b54f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account = \"lab94290\" \n",
    "container = \"submissions\"\n",
    "group = \"itay_asaf_antal\"\n",
    "sas_token = \"...\" # change to your sas token\n",
    "sas_token = sas_token.lstrip('?')\n",
    "\n",
    "# 2) tell ABFS to use SAS for the account and give the fixed token provider\n",
    "acct = storage_account\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{acct}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{acct}.dfs.core.windows.net\",\n",
    "               \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{acct}.dfs.core.windows.net\", sas_token)\n",
    "\n",
    "output_path = f\"abfss://{container}@{acct}.dfs.core.windows.net/{group}/scraped_expedia.csv\"\n",
    "\n",
    "scraped_expedia = (spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .option(\"escape\", '\"')\n",
    "    .option(\"quote\", '\"')\n",
    "    .load(output_path)\n",
    ")\n",
    "\n",
    "print(\"number of rows is\", scraped_expedia.count())\n",
    "display(scraped_expedia.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95fb7308-c0cf-4079-9345-9c98376bb052",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"hotel_id\":260},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769612023152}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scraped_expedia = scraped_expedia.select(\"Hotel Name\", \"City\", \"Country\", \"Rating\", \"Review\")\n",
    "\n",
    "scraped_expedia = scraped_expedia.withColumn(\n",
    "    \"hotel_id\",\n",
    "    F.concat_ws(\", \", F.col(\"Hotel name\"), F.col(\"City\"), F.col(\"Country\"))\n",
    ").withColumn(\"hotel_id\", F.lower(F.col(\"hotel_id\")))\n",
    "\n",
    "scraped_expedia = scraped_expedia.select(\"hotel_id\",\n",
    "                F.col(\"Review\").alias(\"text_review\"),\n",
    "                F.col(\"Rating\").alias(\"label\")\n",
    "                )\n",
    "display(scraped_expedia.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04ae7e18-02f9-411a-b205-add787614c64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### combined scraped dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c5e2279-b437-4a03-a45e-a5e2344b63c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_scraped = scraped_booking.unionByName(scraped_expedia)\n",
    "print(\"number of rows is\", df_scraped.count())\n",
    "display(df_scraped.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56fa863b-8d84-407e-82ee-26661711715f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate Null and NaN counts for all columns in df_scraped\n",
    "null_counts_df = df_scraped.select([\n",
    "    count(when(col(c).isNull() | isnan(c), c)).alias(c) \n",
    "    for c in df_scraped.columns\n",
    "])\n",
    "\n",
    "# Display the results\n",
    "print(\"Null/NaN counts for each column in df_scraped:\")\n",
    "null_counts_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66db79f9-401d-48c3-9900-8d4bd367ca98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_short_reviews = df_scraped.filter(\n",
    "    F.length(F.trim(F.col(\"text_review\"))) < 20\n",
    ")\n",
    "\n",
    "print(\"number of rows is\", df_short_reviews.count())\n",
    "df_short_reviews.select(\"text_review\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e9cb176-25fe-4fb9-abc6-4e60fffaf0e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set seed to ensure consistent language detection results\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# DEFINE THE ENGLISH DETECTION UDF\n",
    "@F.udf(returnType=StringType())\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"error\"\n",
    "\n",
    "# Pattern: Anything NOT (a-z, A-Z, 0-9, space, !, ,, ., ?, -, (, ))\n",
    "strict_pattern = r\"[^a-zA-Z0-9\\s\\!\\,\\.\\?\\-\\(\\)]\"\n",
    "    \n",
    "def clean_and_filter_english_reviews(df, text_review_col):\n",
    "    \"\"\"\n",
    "    Cleans review text by removing unwanted characters and filters out short or non-English reviews.\n",
    "    Keeps only English reviews with sufficient length and returns a cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    df = (\n",
    "        df.dropna(subset=[text_review_col])\n",
    "          .withColumn(\"cleaned_review\", F.regexp_replace(F.col(text_review_col), strict_pattern, \"\"))\n",
    "          .filter(F.length(F.trim(F.col(\"cleaned_review\"))) >= 20)\n",
    "          .withColumn(\"language\", detect_language(F.col(\"cleaned_review\")))\n",
    "          .filter(F.col(\"language\") == \"en\")\n",
    "          .drop(text_review_col, \"language\")\n",
    "          .withColumnRenamed(\"cleaned_review\", text_review_col)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df_scraped = clean_and_filter_english_reviews(df_scraped, \"text_review\")\n",
    "\n",
    "print(f\"after cleaning and filter en, row count is: {df_scraped.count()}\")\n",
    "display(df_scraped.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb1963b3-0f57-4a66-98da-7680d2d99e44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_scraped = (\n",
    "    df_scraped\n",
    "    .withColumn(\"label_d\", F.col(\"label\").cast(\"double\"))\n",
    "    .filter(F.col(\"label_d\").isNotNull())\n",
    "    .withColumn(\"label\", F.round(\"label_d\"))\n",
    "    .filter(F.col(\"label\").between(1, 10))\n",
    "    .drop(\"label_d\")\n",
    ")\n",
    "print(\"after ensure label is double, row count is:\", df_scraped.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b19aa7b-a22c-43cd-9e04-ae19d75bf300",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_scraped = df_scraped.dropDuplicates()\n",
    "print(\"after drop duplicate, row count is:\", df_scraped.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bab3775-de1f-46dc-a18e-714cbd723d8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**for EDA:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e9e7542-e013-4fb2-bd38-dc620275a252",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sample_scraped = df_scraped.sample(withReplacement=False, fraction=0.3, seed=42).cache()\n",
    "total = df_sample_scraped.count()\n",
    "print(\"total amount of sample for eda:\", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70dbe3fa-f092-4291-ab7f-35f379557c7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_counts = (df_sample_scraped\n",
    "    .groupBy(\"label\")\n",
    "    .count()\n",
    "    .withColumn(\"percent\", F.col(\"count\") / F.lit(total) * 100)\n",
    "    .orderBy(\"label\")\n",
    ")\n",
    "\n",
    "pdf = df_counts.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3aafb33b-e778-4bb7-8dcb-d35dc1ca9ac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "bars = plt.bar(pdf[\"label\"], pdf[\"percent\"])\n",
    "\n",
    "plt.xticks(range(1, 11))\n",
    "plt.xlim(0.5, 10.5)\n",
    "\n",
    "plt.xlabel(\"rating\")\n",
    "plt.ylabel(\"percent (%)\")\n",
    "plt.title(\"Rating Distribution of Full Reviews\")\n",
    "\n",
    "for bar in bars:\n",
    "    h = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width()/2,\n",
    "        h + 0.3,                 # המרחק מעל העמודה (שנה ל-0.2/0.5 לפי טעם)\n",
    "        f\"{h:.2f}%\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=9\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba92e3a9-d2cd-4f76-8f88-c0b5643d1f8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### scraped booking real categories scores dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "877cd02e-14dc-4abd-89e3-5a331933412f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769617389969}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account = \"lab94290\" \n",
    "container = \"submissions\"\n",
    "group = \"itay_asaf_antal\"\n",
    "sas_token = \"...\" # change to your sas token\n",
    "sas_token = sas_token.lstrip('?')\n",
    "\n",
    "# 2) tell ABFS to use SAS for the account and give the fixed token provider\n",
    "acct = storage_account\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{acct}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{acct}.dfs.core.windows.net\",\n",
    "               \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{acct}.dfs.core.windows.net\", sas_token)\n",
    "\n",
    "output_path = f\"abfss://{container}@{acct}.dfs.core.windows.net/{group}/scraped_booking_real_scores.csv\"\n",
    "\n",
    "scraped_booking_real_scores = (spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .option(\"escape\", '\"')\n",
    "    .option(\"quote\", '\"')\n",
    "    .load(output_path)\n",
    ")\n",
    "\n",
    "print(\"number of rows is\", scraped_booking_real_scores.count())\n",
    "display(scraped_booking_real_scores.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15cbfb99-e8c0-43d9-9800-4e98186101c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scraped_booking_real_scores = scraped_booking_real_scores.withColumn(\n",
    "    \"hotel_id\",\n",
    "    F.concat_ws(\", \", F.col(\"HotelName\"), F.col(\"City\"), F.col(\"Country\"))\n",
    ").withColumn(\"hotel_id\", F.lower(F.col(\"hotel_id\")))\n",
    "\n",
    "scraped_booking_real_scores = scraped_booking_real_scores.drop(\"HotelName\", \"City\", \"Country\")\n",
    "\n",
    "scraped_booking_real_scores = scraped_booking_real_scores.toDF(*[c.lower() for c in scraped_booking_real_scores.columns])\n",
    "scraped_booking_real_scores = scraped_booking_real_scores.dropDuplicates([\"hotel_id\"])\n",
    "\n",
    "print(\"number of rows is\", scraped_booking_real_scores.count())\n",
    "display(scraped_booking_real_scores.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2421be9-3f0a-4617-9e47-c9c4c0bf9a06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### df train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb7911f-4420-4b79-9dd6-c22cf62aa695",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train = df_scraped.join(\n",
    "    broadcast(scraped_booking_real_scores.select(\"hotel_id\").distinct()),\n",
    "    on=\"hotel_id\",\n",
    "    how=\"left_anti\"\n",
    ")\n",
    "\n",
    "print(\"number of rows is\", df_train.count())\n",
    "display(df_train.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cc5b7b9-2ec8-4f3f-8dcf-996e339d3850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train = df_train.sample(withReplacement=False, fraction=0.7, seed=42).cache()\n",
    "print(\"number of rows is\", df_train.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f332807-7f6c-4fa4-a5be-1126cf03e963",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Predicts sentiment (positive / negative) for each hotel review using a BERT model\n",
    "# fine-tuned on hotel-review data. Internal labels are mapped to readable sentiment.\n",
    "\n",
    "document = (DocumentAssembler()\n",
    "    .setInputCol(\"text\")\n",
    "    .setOutputCol(\"document\"))\n",
    "\n",
    "tokenizer = (Tokenizer()\n",
    "    .setInputCols([\"document\"])\n",
    "    .setOutputCol(\"token\"))\n",
    "\n",
    "clf = (BertForSequenceClassification\n",
    "    .pretrained(\"finetunedmodel_hotel_sentiment_5k\", \"en\")\n",
    "    .setInputCols([\"document\", \"token\"])\n",
    "    .setOutputCol(\"class\"))\n",
    "\n",
    "pipeline = Pipeline(stages=[document, tokenizer, clf])\n",
    "\n",
    "# Dummy fit to materialize the Spark ML pipeline\n",
    "dummy = spark.createDataFrame([(\"ok\",)], [\"text\"])\n",
    "model = pipeline.fit(dummy)\n",
    "\n",
    "def add_sentiment_to_review(df, text_col=\"text_review\"):\n",
    "    \"\"\"\n",
    "    Add a 'sentiment' column (\"positive\"/\"negative\") based on hotel review text.\n",
    "    \"\"\"\n",
    "    tmp = df.withColumn(\"text\", F.col(text_col))\n",
    "    out = model.transform(tmp)\n",
    "    return (\n",
    "        out.select(\n",
    "            *df.columns,\n",
    "            F.col(\"class.result\")[0].alias(\"lbl\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"sentiment\",\n",
    "            F.when(F.col(\"lbl\") == \"LABEL_1\", F.lit(\"negative\"))\n",
    "             .when(F.col(\"lbl\") == \"LABEL_0\", F.lit(\"positive\"))\n",
    "             .otherwise(F.lit(None))\n",
    "        )\n",
    "        .drop(\"lbl\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a03f162-affb-43ac-a8d1-bbac8ffe8866",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_sentiment = add_sentiment_to_review(df_train, text_col=\"text_review\")\n",
    "\n",
    "condition_mismatch = (\n",
    "    ((F.col(\"sentiment\") == \"negative\") & (F.col(\"label\") == 10.0)) |\n",
    "    ((F.col(\"sentiment\") == \"positive\") & (F.col(\"label\") == 1.0))\n",
    ")\n",
    "\n",
    "df_wrong_label = df_with_sentiment.filter(condition_mismatch == True).select(\"text_review\", \"label\", \"sentiment\")\n",
    "print(\"Drop wrong label, count drop rows is:\", df_wrong_label.count())\n",
    "print(\"Exapmles of wrong label:\")\n",
    "display(df_wrong_label.limit(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5717fe2f-2de7-4aeb-a173-23a5bb900b0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train = df_with_sentiment.filter(~condition_mismatch).drop(\"sentiment\").cache()\n",
    "print(\"train df count row is:\", df_train.count())\n",
    "display(df_train.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa32aa08-6648-428a-bd4a-7e2e9806c689",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### df test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cf56c89-9298-45ec-9828-4f242300f98e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "original booking dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6613a2cd-6416-4545-b9f0-aaa4773f7e1c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{\"#row_number#\":true,\"availability\":false,\"city\":true,\"coordinates\":false,\"country\":true,\"description\":false,\"fine_print\":false,\"hotel_id\":true,\"house_rules\":false,\"images\":false,\"location\":false,\"managed_by\":false,\"manager_language_spoken\":false,\"manager_score\":false,\"metro_railway_access\":false,\"most_popular_facilities\":false,\"number_of_reviews\":false,\"popular_facilities\":false,\"property_highlights\":false,\"property_information\":false,\"property_surroundings\":false,\"review_score\":false,\"reviews_scores\":false,\"title\":true,\"top_reviews\":true,\"url\":false}},\"settings\":{\"columns\":{\"url\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}}}},\"syncTimestamp\":1769789858925}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account = \"lab94290\" \n",
    "container = \"booking\"\n",
    "booking_sas_token= \"...\" # change to your sas token\n",
    "booking_sas_token = booking_sas_token.lstrip('?')\n",
    "\n",
    "# 2) tell ABFS to use SAS for the account and give the fixed token provider\n",
    "acct = storage_account\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{acct}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{acct}.dfs.core.windows.net\",\n",
    "               \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{acct}.dfs.core.windows.net\", booking_sas_token)\n",
    "\n",
    "path = f\"abfss://{container}@{acct}.dfs.core.windows.net/booking_1_9.parquet\"\n",
    "df_origin_booking = spark.read.parquet(path)\n",
    "print(\"booking original df count row is:\", df_origin_booking.count())\n",
    "display(df_origin_booking.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64ccadec-bd0d-48e1-9c7b-d351dbfe4d1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sample_origin_booking = df_origin_booking.sample(withReplacement=False, fraction=0.03, seed=42).cache()\n",
    "print(\"sample of booking original df count row is:\", df_sample_origin_booking.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "322efd7b-b034-4344-bfbd-7044d7dbaa9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sample_origin_booking = df_sample_origin_booking.select(\"city\", \"country\", \"hotel_id\", \"title\", \"top_reviews\")\n",
    "\n",
    "df_sample_origin_booking = df_sample_origin_booking.filter(F.size(F.col(\"top_reviews\")) > 0)\n",
    "print(\"row count is:\", df_sample_origin_booking.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84b1d933-4b0b-41c6-bbf2-2f96b1239c1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Identify hotel_ids that have more than 1 distinct title\n",
    "bad_hotels_df = df_sample_origin_booking.groupBy(\"hotel_id\") \\\n",
    "    .agg(F.countDistinct(\"title\").alias(\"name_count\")) \\\n",
    "    .filter(F.col(\"name_count\") >= 2) \\\n",
    "    .select(\"hotel_id\")\n",
    "\n",
    "# 2. Use a Left Anti Join to remove every row belonging to those IDs\n",
    "booking_df_cleaned = df_sample_origin_booking.join(bad_hotels_df, on=\"hotel_id\", how=\"left_anti\")\n",
    "\n",
    "# 3. Verification\n",
    "original_ids = df_sample_origin_booking.select(\"hotel_id\").distinct().count()\n",
    "final_ids = booking_df_cleaned.select(\"hotel_id\").distinct().count()\n",
    "\n",
    "print(f\"Removed {original_ids - final_ids} hotel IDs that had inconsistent names.\")\n",
    "print(f\"Remaining unique hotel IDs: {final_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74952202-9d02-4e85-9bc5-399e6f52ee6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate Null and NaN counts for all columns\n",
    "# For complex types (arrays, structs), only check isNull()\n",
    "# For other types, check both isNull() and isnan() after casting to double\n",
    "null_counts = booking_df_cleaned.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c)\n",
    "    if str(booking_df_cleaned.schema[c].dataType).startswith('ArrayType') or\n",
    "       str(booking_df_cleaned.schema[c].dataType).startswith('StructType')\n",
    "    else count(when(col(c).isNull() | (col(c).cast('double').isNotNull() & isnan(col(c).cast('double'))), c)).alias(c)\n",
    "    for c in booking_df_cleaned.columns\n",
    "])\n",
    "\n",
    "print(\"Missing Values (Null/NaN) per column:\")\n",
    "null_counts.show()\n",
    "\n",
    "initial_count = booking_df_cleaned.count()\n",
    "\n",
    "booking_df_cleaned = booking_df_cleaned.dropna()\n",
    "\n",
    "after_dropna_count = booking_df_cleaned.count()\n",
    "print(f\"Row count after dropping nulls: {after_dropna_count}\")\n",
    "\n",
    "booking_df_cleaned = booking_df_cleaned.dropDuplicates()\n",
    "\n",
    "final_count = booking_df_cleaned.count()\n",
    "\n",
    "print(f\"Original Row Count: {initial_count}\")\n",
    "print(f\"Duplicates Removed: {after_dropna_count - final_count}\")\n",
    "print(f\"Final Row Count: {final_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d09d1f5e-977b-44d1-b7fb-338c8356ee35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "booking_df_cleaned = booking_df_cleaned.withColumn(\n",
    "    \"hotel_name\",\n",
    "    F.concat_ws(\", \", F.col(\"title\"), F.col(\"city\"), F.col(\"country\"))\n",
    ").withColumn(\"hotel_name\", F.lower(F.col(\"hotel_name\")))\n",
    "\n",
    "booking_df_cleaned = booking_df_cleaned.select(\"hotel_id\", \"hotel_name\", \"top_reviews\")\n",
    "\n",
    "# 2. Explode the array of dictionaries\n",
    "# Each row currently has an array like [{\"review\": \"...\", ...}, {\"review\": \"...\", ...}]\n",
    "df_exploded = booking_df_cleaned.withColumn(\"review_dict\", F.explode(F.col(\"top_reviews\")))\n",
    "\n",
    "# 3. Extract the 'review' field and Normalize\n",
    "# We lowercase immediately to make regex matching easier\n",
    "df_extracted = df_exploded.select(\n",
    "    F.col(\"hotel_name\").alias(\"hotel_id\"),\n",
    "    F.lower(F.col(\"review_dict.review\")).alias(\"text_review\")\n",
    ")\n",
    "print(\"row count is:\", df_extracted.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b48b01e8-cd3b-4cb6-b5f4-5f608f2af3cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sample_origin_booking = clean_and_filter_english_reviews(df_extracted, \"text_review\")\n",
    "print(\"row count is:\", df_sample_origin_booking.count())\n",
    "display(df_sample_origin_booking.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d2d70db-7225-4c68-bc38-98332cbac4d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "original airbnb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a732cdf-ad76-4dcd-8ab6-9cadd069234f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{\"#row_number#\":true,\"name\":false,\"price\":false,\"image\":false,\"description\":false,\"category\":false,\"availability\":false,\"discount\":false,\"reviews\":true,\"ratings\":false,\"seller_info\":false,\"breadcrumbs\":false,\"location\":false,\"lat\":false,\"long\":false,\"guests\":false,\"pets_allowed\":false,\"description_items\":false,\"category_rating\":false,\"house_rules\":false,\"details\":false,\"highlights\":false,\"arrangement_details\":false,\"amenities\":false,\"images\":false,\"available_dates\":false,\"url\":false,\"final_url\":false,\"listing_title\":false,\"property_id\":false,\"listing_name\":false,\"location_details\":false,\"description_by_sections\":false,\"description_html\":false,\"location_details_html\":false,\"is_supperhost\":false,\"host_number_of_reviews\":false,\"host_rating\":false,\"hosts_year\":false,\"host_response_rate\":false,\"is_guest_favorite\":false,\"travel_details\":false,\"pricing_details\":false,\"total_price\":false,\"currency\":false,\"cancellation_policy\":false,\"property_number_of_reviews\":false,\"country\":false,\"postcode_map_url\":false,\"host_image\":false,\"host_details\":false}},\"settings\":{\"columns\":{\"image\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}},\"url\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}},\"final_url\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}},\"host_image\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}}}},\"syncTimestamp\":1769793446914}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account = \"lab94290\"  \n",
    "container = \"airbnb\"\n",
    "airbnb_sas_token=\"...\" # change to your sas token\n",
    "sas_token = airbnb_sas_token.lstrip('?')\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{storage_account}.dfs.core.windows.net\", sas_token)\n",
    "path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/airbnb_1_12_parquet\"\n",
    "df_origin_airbnb = spark.read.parquet(path)\n",
    "print(\"airbnb original df count row is:\", df_origin_airbnb.count())\n",
    "display(df_origin_airbnb.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4016514d-8f76-4166-90c8-8cfb11c3058b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sample_origin_airbnb = df_origin_airbnb.sample(withReplacement=False, fraction=0.02, seed=42).cache()\n",
    "print(\"sample of airbnb original df count row is:\", df_sample_origin_airbnb.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b88d1cf-5ea3-4a07-9088-ba3681c1655b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "airbnb_df = df_sample_origin_airbnb.select(\"name\", \"reviews\", \"location\", \"property_id\")\n",
    "\n",
    "airbnb_df = airbnb_df.withColumn(\n",
    "    \"reviews\",\n",
    "    F.from_json(F.col(\"reviews\"), ArrayType(StringType()))\n",
    ")\n",
    "\n",
    "airbnb_df = airbnb_df.filter(F.size(F.col(\"reviews\")) > 0)\n",
    "\n",
    "print(\"row count is:\", airbnb_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2040dd5-5346-47f9-8173-4af1945237fb",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769793594675}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Identify the \"bad\" IDs (those with more than 1 distinct name)\n",
    "bad_ids_df = airbnb_df.groupBy(\"property_id\") \\\n",
    "    .agg(F.countDistinct(\"name\").alias(\"unique_name_count\")) \\\n",
    "    .filter(F.col(\"unique_name_count\") >= 2) \\\n",
    "    .select(\"property_id\")\n",
    "\n",
    "# 2. Remove all rows matching these IDs using a Left Anti Join\n",
    "# This keeps only rows in df_expedia where property_id is NOT in bad_ids_df\n",
    "df_airbnb_cleaned = airbnb_df.join(bad_ids_df, on=\"property_id\", how=\"left_anti\")\n",
    "\n",
    "# 3. Verify the results\n",
    "original_count = airbnb_df.count()\n",
    "final_count = df_airbnb_cleaned.count()\n",
    "removed_rows = original_count - final_count\n",
    "\n",
    "print(f\"Original record count: {original_count}\")\n",
    "print(f\"Records removed: {removed_rows}\")\n",
    "print(f\"Final record count: {final_count}\")\n",
    "\n",
    "display(df_airbnb_cleaned.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0951a2f3-676c-4b3b-9ae6-96f2b241e49d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, StructType, MapType, StringType\n",
    "\n",
    "null_summary = df_airbnb_cleaned.select([\n",
    "    F.count(\n",
    "        F.when(\n",
    "            (F.col(c).isNull()) |\n",
    "            (F.size(F.col(c)) == 0) if isinstance(df_airbnb_cleaned.schema[c].dataType, ArrayType)\n",
    "            else (\n",
    "                (F.trim(F.col(c)) == \"\") if isinstance(df_airbnb_cleaned.schema[c].dataType, StringType)\n",
    "                else (F.isnan(F.col(c).cast(\"double\")))\n",
    "            ),\n",
    "            c\n",
    "        )\n",
    "    ).alias(c)\n",
    "    for c in df_airbnb_cleaned.columns\n",
    "])\n",
    "\n",
    "print(\"Missing values (Null/NaN/Empty) per column:\")\n",
    "null_summary.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fdcb85e-a582-484a-a458-1ba0a3dc26f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# --- 1. DEFINE NORMALIZATION UDF ---\n",
    "# This converts accented characters to their closest English letters\n",
    "@F.udf(returnType=StringType())\n",
    "def normalize_udf(text):\n",
    "    if text is None: return None\n",
    "    # Decompose unicode characters and filter out the accent marks (Mn category)\n",
    "    return \"\".join(\n",
    "        char for char in unicodedata.normalize('NFD', text)\n",
    "        if unicodedata.category(char) != 'Mn'\n",
    "    )\n",
    "\n",
    "# --- 2. MASTER COUNTRY LIST ---\n",
    "# Comprehensive list of countries in lowercase for filtering\n",
    "valid_countries = [\n",
    "    \"afghanistan\", \"albania\", \"algeria\", \"andorra\", \"angola\", \"antigua and barbuda\", \"argentina\", \"armenia\", \"australia\", \n",
    "    \"austria\", \"azerbaijan\", \"bahamas\", \"bahrain\", \"bangladesh\", \"barbados\", \"belarus\", \"belgium\", \"belize\", \"benin\", \n",
    "    \"bhutan\", \"bolivia\", \"bosnia and herzegovina\", \"botswana\", \"brazil\", \"brunei\", \"bulgaria\", \"burkina faso\", \"burundi\", \n",
    "    \"cote d ivoire\", \"cabo verde\", \"cambodia\", \"cameroon\", \"canada\", \"central african republic\", \"chad\", \"chile\", \"china\", \n",
    "    \"colombia\", \"comoros\", \"congo\", \"costa rica\", \"croatia\", \"cuba\", \"cyprus\", \"czechia\", \"czech republic\", \n",
    "    \"democratic republic of the congo\", \"denmark\", \"djibouti\", \"dominica\", \"dominican republic\", \"ecuador\", \"egypt\", \n",
    "    \"el salvador\", \"equatorial guinea\", \"eritrea\", \"estonia\", \"eswatini\", \"ethiopia\", \"fiji\", \"finland\", \"france\", \"gabon\", \n",
    "    \"gambia\", \"georgia\", \"germany\", \"ghana\", \"greece\", \"grenada\", \"guatemala\", \"guinea\", \"guinea bissau\", \"guyana\", \"haiti\", \n",
    "    \"honduras\", \"hungary\", \"iceland\", \"india\", \"indonesia\", \"iran\", \"iraq\", \"ireland\", \"israel\", \"italy\", \"jamaica\", \"japan\", \n",
    "    \"jordan\", \"kazakhstan\", \"kenya\", \"kiribati\", \"kuwait\", \"kyrgyzstan\", \"laos\", \"latvia\", \"lebanon\", \"lesotho\", \"liberia\", \n",
    "    \"libya\", \"liechtenstein\", \"lithuania\", \"luxembourg\", \"madagascar\", \"malawi\", \"malaysia\", \"maldives\", \"mali\", \"malta\", \n",
    "    \"marshall islands\", \"mauritania\", \"mauritius\", \"mexico\", \"micronesia\", \"moldova\", \"monaco\", \"mongolia\", \"montenegro\", \n",
    "    \"morocco\", \"mozambique\", \"myanmar\", \"namibia\", \"nauru\", \"nepal\", \"netherlands\", \"new zealand\", \"nicaragua\", \"niger\", \n",
    "    \"nigeria\", \"north korea\", \"north macedonia\", \"norway\", \"oman\", \"pakistan\", \"palau\", \"palestine\", \"panama\", \n",
    "    \"papua new guinea\", \"paraguay\", \"peru\", \"philippines\", \"poland\", \"portugal\", \"qatar\", \"romania\", \"russia\", \"rwanda\", \n",
    "    \"saint kitts and nevis\", \"saint lucia\", \"saint vincent and the grenadines\", \"samoa\", \"san marino\", \"sao tome and principe\", \n",
    "    \"saudi arabia\", \"senegal\", \"serbia\", \"seychelles\", \"sierra leone\", \"singapore\", \"slovakia\", \"slovenia\", \"solomon islands\", \n",
    "    \"somalia\", \"south africa\", \"south korea\", \"south sudan\", \"spain\", \"sri lanka\", \"sudan\", \"suriname\", \"sweden\", \n",
    "    \"switzerland\", \"syria\", \"tajikistan\", \"tanzania\", \"thailand\", \"timor leste\", \"togo\", \"tonga\", \"trinidad and tobago\", \n",
    "    \"tunisia\", \"turkey\", \"turkmenistan\", \"tuvalu\", \"uganda\", \"ukraine\", \"united arab emirates\", \"united kingdom\", \"uk\", \n",
    "    \"united states\", \"usa\", \"united states of america\", \"uruguay\", \"uzbekistan\", \"vanuatu\", \"venezuela\", \"vietnam\", \n",
    "    \"yemen\", \"zambia\", \"zimbabwe\"\n",
    "]\n",
    "\n",
    "# --- 3. EXTRACTION & FILTERING LOGIC ---\n",
    "\n",
    "# Step A: Split the location by comma and extract the last part\n",
    "# Step B: Lowercase, Normalize accents, and remove non-letter characters\n",
    "# Step C: Filter rows based on the valid_countries list or empty values\n",
    "df_airbnb_final = df_airbnb_cleaned\n",
    "df_airbnb_final = df_airbnb_final.withColumn(\n",
    "    \"country_temp\", \n",
    "    F.element_at(F.split(F.col(\"location\"), \",\"), -1)\n",
    ").withColumn(\n",
    "    \"country\",\n",
    "    F.trim(\n",
    "        F.regexp_replace(\n",
    "            F.regexp_replace(normalize_udf(F.lower(F.col(\"country_temp\"))), r\"[^a-z\\s]\", \" \"),\n",
    "            r\"\\s+\", \" \"\n",
    "        )\n",
    "    )\n",
    ").filter(\n",
    "    (F.col(\"country\").isin(valid_countries)) | \n",
    "    (F.col(\"country\") == \"\") | \n",
    "    (F.col(\"country\").isNull())\n",
    ").drop(\"country_temp\") # Ditching intermediate extraction columns\n",
    "\n",
    "# --- 4. VERIFICATION ---\n",
    "print(f\"Extraction and filtering complete.\")\n",
    "df_airbnb_final.select(\"location\", \"country\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1866a084-ffb4-428b-9280-9ad1da8ef29b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. Split by the middle dot character \"·\"\n",
    "# 2. Extract the first element (index 0)\n",
    "# 3. Trim any leftover whitespace\n",
    "df_airbnb_final = df_airbnb_final.withColumn(\n",
    "    \"property_name_extracted\", \n",
    "    F.trim(F.split(F.col(\"name\"), \"·\").getItem(0))\n",
    ")\n",
    "\n",
    "# Optional: Replace the original name column with the extracted one\n",
    "df_airbnb_final = df_airbnb_final.drop(\"name\").withColumnRenamed(\"property_name_extracted\", \"name\")\n",
    "\n",
    "# Verification\n",
    "df_airbnb_final.select(\"name\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acdb44e1-7e5c-4a69-aa33-738eee234f7e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"hotel_id\":212},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769794517096}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_airbnb_final = df_airbnb_final.drop(\"location\")\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. Clean the outer quotes from the start and end of the string\n",
    "# 2. Split by the 3-character delimiter \",\"\n",
    "# 3. Explode the resulting array into multiple rows\n",
    "df_airbnb_exploded = df_airbnb_final.withColumn(\n",
    "    \"text_review\", \n",
    "    F.explode(F.col(\"reviews\"))\n",
    ").withColumn(\n",
    "    \"hotel_id\",\n",
    "    F.concat_ws(\", \", F.col(\"name\"), F.col(\"country\"))\n",
    ").withColumn(\"hotel_id\", F.lower(F.col(\"hotel_id\")))\n",
    "\n",
    "\n",
    "# 4. Final cleaning: remove the array column and show results\n",
    "df_airbnb_final = df_airbnb_exploded.select(\"hotel_id\", \"text_review\")\n",
    "print(\"row count is:\", df_airbnb_final.count())\n",
    "display(df_airbnb_final.limit(10))                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ffc2312-e532-46ff-b2b2-8d6e7bb7b996",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_airbnb_final = df_airbnb_final.sample(withReplacement=False, fraction=0.1, seed=42)\n",
    "print(\"sample of airbnb final df count row is:\", df_airbnb_final.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6c14e28-e944-4618-a421-822f0b92d901",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sample_origin_airbnb = clean_and_filter_english_reviews(df_airbnb_final, \"text_review\")\n",
    "print(\"row count is:\", df_sample_origin_airbnb.count())\n",
    "display(df_sample_origin_airbnb.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e666a61-0c6f-4a96-bd5d-585cbfbdd597",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**union dfs for df_test:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5273999d-ae57-4f25-8689-cc3c8f8af85b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_scraped_for_test = df_scraped.drop(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6986cd76-21b1-49f7-9e41-503cfc0616fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_test = df_scraped_for_test.unionByName(df_sample_origin_booking)\n",
    "df_test = df_test.unionByName(df_sample_origin_airbnb)\n",
    "df_test = df_test.dropDuplicates().cache()\n",
    "print(\"number of rows is\", df_test.count())\n",
    "display(df_test.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f139237-f4c2-4598-a96f-e540dce8f7e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10ffb5fb-3586-4c41-b90c-5ed7f532a16c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**For training linear regression:**\n",
    "- Split into categories (each review can be in multiple categories).\n",
    "- Trim according to category relevant parts.\n",
    "- align review score according to sentiment analysis.\n",
    "- Build linear regression model for each category from train set with noise (X is embbeding of review and Y is its score).\n",
    "\n",
    "**For final hotel's category score:**\n",
    "- For a review find all categoires its about.\n",
    "- Trim each review to relevent category parts.\n",
    "- Seperatly find predictions with linear regression model and avg per hotel_id save it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08bbc759-af8e-4e82-8169-738c16d67cf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Split into Categories and Refine Review Score:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "307273f4-4966-4498-88c7-e8b5f5b92229",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Find Review Categories:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdb12e22-3d80-4ebb-9521-cfd2c818b218",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "categories_kw = {\n",
    "  \"cleanliness\": [\n",
    "    \"clean\",\"cleanliness\",\"spotless\",\"tidy\",\"neat\",\"hygienic\",\"sanitary\",\n",
    "    \"dirty\",\"dusty\",\"dust\",\"grime\",\"stain\",\"stained\",\"smell\",\"odor\",\"mould\",\"mold\",\"mildew\",\n",
    "    \"housekeeping\",\"clean towels\",\"clean sheets\",\"fresh linens\",\"not cleaned\",\n",
    "    \"filthy\", \"unclean\", \"smelly\", \"stinky\", \"musty\", \"damp\", \"smoke\", \"smoky\",\n",
    "    \"bugs\", \"insects\", \"cockroaches\", \"ants\", \"hair\", \"hairs\",\n",
    "    \"immaculate\",\"pristine\",\"sparkling\",\"well kept\",\"well-kept\",\"sanitized\",\"sanitised\",\n",
    "    \"cleaned\",\"not clean\",\"not cleaned properly\",\"deep clean\",\"deep-clean\",\"fresh smell\",\n",
    "    \"reek\",\"reeking\",\"stink\",\"stinky smell\",\"sewage\",\"drain smell\",\n",
    "    \"sticky\",\"sticky floor\",\"greasy\",\"grease\",\"slimy\",\"moldy\",\"mouldy\",\n",
    "    \"cobweb\",\"cobwebs\",\"bedbugs\",\"bed bugs\",\"mosquitoes\",\"flies\",\"spider\",\"spiders\",\n",
    "    \"vacuum\",\"vacuumed\",\"mop\",\"mopped\",\"trash\",\"garbage\",\"bin\",\"bins\"\n",
    "  ],\n",
    "  \"comfort\": [\n",
    "    \"comfortable\",\"comfort\",\"cozy\",\"snug\", \"comfy\"\n",
    "    \"bed\",\"mattress\",\"pillow\",\"pillows\",\"bedding\",\"sheets\",\"linens\",\n",
    "    \"soft bed\",\"hard bed\",\"uncomfortable\",\"lumpy\",\n",
    "    \"noise\",\"noisy\",\"quiet\",\"soundproof\",\"thin walls\",\n",
    "    \"temperature\",\"hot\",\"cold\",\"heating\",\"heater\",\"air conditioning\",\"ac\",\n",
    "    \"spacious\",\"cramped\",\"small room\",\"room size\", \"comfy\",\n",
    "    \"firm bed\", \"soft mattress\", \"hard mattress\", \"blanket\", \"blankets\", \"duvet\", \"sleep\", \"slept\", \"sleeping\",\n",
    "    \"warm\", \"cool\", \"freezing\", \"aircon\", \"ventilation\",\n",
    "    \"restful\",\"relaxing\",\"good sleep\",\"sleep well\",\"good night sleep\",\n",
    "    \"king bed\",\"queen bed\",\"double bed\",\"single bed\",\"twin bed\",\n",
    "    \"sofa bed\",\"couch\",\"sofa\",\"couch bed\",\"extra bed\",\"rollaway\",\"crib\",\"cot\",\n",
    "    \"squeaky\",\"creaky\",\"bed frame\",\"springs\",\n",
    "    \"blackout\",\"blackout curtains\",\"curtains\",\"blinds\",\n",
    "    \"stuffy\",\"humid\",\"humidity\",\"draught\",\"draft\",\"drafty\",\"draughty\",\n",
    "    \"street noise\",\"traffic noise\",\"construction noise\",\"earplugs\"\n",
    "  ],\n",
    "  \"facilities\": [\n",
    "    \"facilities\",\"amenities\",\"equipment\",\n",
    "    \"gym\",\"fitness\",\"pool\",\"swimming pool\",\"sauna\",\"spa\",\"jacuzzi\",\"hot tub\",\n",
    "    \"elevator\",\"lift\",\"lobby\",\"lounge\",\"terrace\",\"garden\",\"patio\",\n",
    "    \"parking\",\"car park\",\"garage\",\n",
    "    \"restaurant\",\"bar\",\"breakfast area\",\"cafeteria\", \"breakfast\",\n",
    "    \"laundry\",\"washing machine\",\"dryer\",\n",
    "    \"kitchenette\",\"kitchen\",\"microwave\",\"fridge\",\"refrigerator\",\"kettle\",\n",
    "    \"tv\",\"television\",\"channels\", \"bathroom\", \"shower\", \"toilet\",\n",
    "    \"water pressure\", \"hot water\", \"cold water\", \"coffee\", \"tea\", \"coffee machine\",\n",
    "    \"dishwasher\", \"oven\", \"stove\", \"workspace\", \"desk\", \"balcony\",\n",
    "    \"air conditioner\",\"air conditioning unit\",\"heater\",\"radiator\",\n",
    "    \"hairdryer\",\"hair dryer\",\"toiletries\",\"soap\",\"shampoo\",\"conditioner\",\"body wash\",\n",
    "    \"towels\",\"bath towel\",\"bathrobe\",\"robes\",\"slippers\",\n",
    "    \"bathtub\",\"bath tub\",\"sink\",\"drain\",\"bidet\",\n",
    "    \"iron\",\"ironing board\",\"safe\",\"minibar\",\"mini bar\",\n",
    "    \"usb\",\"usb outlet\",\"charger\",\"charging\",\"plug\",\"socket\",\"outlet\",\n",
    "    \"vending machine\",\"ice machine\",\"water dispenser\",\"water cooler\",\n",
    "    \"kids club\",\"playground\",\"game room\",\"games room\",\n",
    "    \"meeting room\",\"conference room\",\"business center\",\n",
    "    \"shuttle\",\"airport shuttle\",\"bike rental\",\"bicycle\"\n",
    "  ],\n",
    "  \"staff\": [\n",
    "    \"staff\",\"service\",\"team\",\"personnel\",\"receptionist\",\"front desk\",\"reception\",\n",
    "    \"helpful\",\"friendly\",\"polite\",\"welcoming\",\"attentive\",\"professional\",\"kind\",\n",
    "    \"rude\",\"unfriendly\",\"unhelpful\",\"disrespectful\",\"arrogant\",\n",
    "    \"check-in\",\"check in\",\"check-out\",\"checkout\",\n",
    "    \"communication\",\"responsive\",\"response time\",\n",
    "    \"host\", \"hosts\", \"manager\", \"owner\", \"support\", \"customer service\", \"greet\", \"greeted\",\n",
    "     \"concierge\",\"porter\",\"bellboy\",\"bellhop\",\"doorman\",\n",
    "    \"courteous\",\"accommodating\",\"helped\",\"assisted\",\"supportive\",\"patient\",\n",
    "    \"ignored\",\"ignoring\",\"impolite\",\"unprofessional\",\n",
    "    \"efficient\",\"inefficient\",\"slow\",\"quick\",\"prompt\",\"delayed\",\n",
    "    \"early check-in\",\"early check in\",\"late check-out\",\"late check out\",\n",
    "    \"upgrade\",\"upgraded\",\"reservation\",\"booking issue\",\"refund\"\n",
    "  ],\n",
    "  \"location\": [\n",
    "    \"location\",\"located\",\"area\",\"neighborhood\",\"neighbourhood\",\n",
    "    \"central\",\"city center\",\"city centre\",\"downtown\",\"in the center\",\n",
    "    \"close to\",\"near\",\"nearby\",\"walking distance\",\"steps away\",\n",
    "    \"transport\",\"public transport\",\"metro\",\"subway\",\"train\",\"bus\",\"station\",\n",
    "    \"safe area\",\"unsafe\",\"sketchy\", \"close by\",\n",
    "    \"convenient\", \"conveniently located\", \"distance\",\n",
    "    \"far\", \"far from\", \"away from\", \"walkable\",\n",
    "    \"near the beach\",\"beach\",\"seafront\",\"waterfront\",\n",
    "    \"old town\",\"city centre\",\"main square\",\n",
    "    \"restaurants nearby\",\"shops nearby\",\"shopping\",\"supermarket\",\"grocery\",\n",
    "    \"attractions\",\"sights\",\"tourist area\",\n",
    "    \"airport\",\"near the airport\",\"port\",\"harbor\",\"harbour\",\n",
    "    \"quiet area\",\"noisy area\",\"busy area\",\n",
    "    \"hill\",\"steep\",\"remote\",\"isolated\"\n",
    "  ],\n",
    "  \"free_wifi\": [\n",
    "    \"wifi\",\"wi-fi\", \"wi fi\", \"wireless\",\"internet\",\"connection\",\"network\",\"router\",\n",
    "    \"password\",\"speed\",\"fast wifi\",\"slow wifi\",\"unstable\",\"disconnect\",\"drops\",\n",
    "    \"no signal\",\"coverage\",\"bandwidth\",\"streaming\",\"zoom\",\"video call\",\"free wifi\",\n",
    "    \"signal\", \"lag\", \"laggy\", \"buffering\", \"connect\", \"connected\",\n",
    "    \"internet access\",\"wifi signal\",\"signal strength\",\"strong signal\",\"weak signal\",\n",
    "    \"login\",\"log in\",\"sign in\",\"portal\",\n",
    "    \"download\",\"upload\",\"mbps\",\"ping\",\"latency\",\n",
    "    \"ethernet\",\"lan\",\"wired internet\",\"modem\",\"hotspot\"\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a36e27b7-19d4-491e-8919-31187749430b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_categories_column(df, text_review_col):\n",
    "    \"\"\"\n",
    "    Creates a 'categories' array column for each review.\n",
    "    Each review is assigned to all categories whose keywords appear in the text.\n",
    "    If no category matches, take out the review and not use it.\n",
    "    \"\"\"\n",
    "    txt = F.lower(F.col(text_review_col))\n",
    "\n",
    "    cat_cols = []\n",
    "    for ctg, kws in categories_kw.items():\n",
    "        is_ctg = None\n",
    "        for k in kws:\n",
    "            cond = txt.contains(k)\n",
    "            is_ctg = cond if is_ctg is None else (is_ctg | cond)\n",
    "\n",
    "        cat_cols.append(F.when(is_ctg, F.lit(ctg)))\n",
    "\n",
    "    df = df.withColumn(\"categories_raw\", F.array(*cat_cols))\n",
    "    df = df.withColumn(\n",
    "        \"categories\",\n",
    "        F.expr(\"filter(categories_raw, x -> x is not null)\")\n",
    "    ).drop(\"categories_raw\")\n",
    "\n",
    "    return df.filter(F.size(\"categories\") > 0) \n",
    "\n",
    "df_train = create_categories_column(df_train, \"text_review\")\n",
    "df_train_long = df_train.select(\n",
    "        \"text_review\",\n",
    "        \"label\",\n",
    "        F.explode(\"categories\").alias(\"category\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0901c69-9c05-4805-b213-97c5157034f9",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768829284946}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_train_long.limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8772e5eb-243a-4078-bb87-f71f39de4010",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Trim Review To Category Relevant Text:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79e6b987-23a3-4486-963b-cb7accaaa26f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cuts each review down to only the sentence parts that belong to the row’s category:\n",
    "# it first splits the review into chunks (by punctuation and contrast words), then keeps only the chunks that contain category keywords, and finally replaces text_review with those kept chunks.\n",
    "\n",
    "# Convert the multi-label categories column into separate training DataFrames,\n",
    "#       one DataFrame per category (each review may appear in multiple category datasets)\n",
    "\n",
    "contrast_words = [\n",
    "    \"but\", \"however\", \"though\", \"although\", \"yet\", \"whereas\", \"while\", \"on the other hand\",\n",
    "    \"even though\", \"even if\", \"still\", \"nevertheless\", \"nonetheless\", \"except\", \"except for\",\n",
    "    \"apart from\", \"aside from\",\"instead\", \"otherwise\", \"rather\", \"rather than\", \"despite\",\n",
    "    \"in spite of\", \"regardless\", \"regardless of\", \"unfortunately\", \"sadly\",\n",
    "]\n",
    "contrast_regex = \"|\".join(contrast_words)\n",
    "\n",
    "kws_words = [kw for kws in categories_kw.values() for kw in kws]\n",
    "kws_regex = \"|\".join(kw.replace(\" \", r\"\\s+\") for kw in kws_words)\n",
    "\n",
    "split_regex = (\n",
    "    r\"(?<=\\.{3})\\s+\"\n",
    "    r\"|(?<=[.!?])\\s+\"\n",
    "    r\"|\\s+(?:\" + contrast_regex +r\")\\s+\"\n",
    "    r\"|(?:\\s+and\\s+|,\\s*)\"\n",
    "      r\"(?=(?:(?!\\s+and\\s+|,\\s*).)*\\b(?:\" + kws_regex + r\")\\b)\"\n",
    ")\n",
    "\n",
    "def trim_review_to_category_relevant_text(df):\n",
    "    \"\"\"\n",
    "    First splits each review into chunks using sentence punctuation and contrast words.\n",
    "    Then conditionally splits on \"and\"/commas only when the following phrase contains a category keyword, and keeps only the chunks relevant to each category.\n",
    "    \"\"\"\n",
    "    category_dfs = {}\n",
    "\n",
    "    for ctg, kws in categories_kw.items():\n",
    "        kws_lower = [k.lower() for k in kws]\n",
    "\n",
    "        patterns = []\n",
    "        for k in kws_lower:\n",
    "            k_esc = re.escape(k)\n",
    "            k_esc = k_esc.replace(r\"\\ \", r\"\\s+\")\n",
    "            patterns.append(f\"rlike(s, '\\\\\\\\b{k_esc}\\\\\\\\b')\")\n",
    "\n",
    "        hits_sql = \" OR \".join(patterns) if patterns else \"false\"\n",
    "\n",
    "        category_dfs[ctg] = (\n",
    "            df\n",
    "            .filter(F.col(\"category\") == ctg)\n",
    "            .withColumn(\"_segments\", F.split(F.lower(\"text_review\"), split_regex))\n",
    "            .withColumn(\"_segments\", F.expr(\"filter(_segments, s -> trim(s) <> '')\"))\n",
    "            .withColumn(\n",
    "                \"_hits\",\n",
    "                F.expr(f\"filter(_segments, s -> ({hits_sql}))\")\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"_hits\",\n",
    "                F.expr(\"transform(_hits, s -> regexp_replace(trim(s), '[.!?]+$', ''))\")\n",
    "            )\n",
    "            .withColumn(\"text_review\", F.concat_ws(\". \", F.col(\"_hits\")))\n",
    "            .drop(\"_segments\", \"_hits\")\n",
    "            .filter(F.length(\"text_review\") > 0)\n",
    "            .select(*[c for c in [\"hotel_id\", \"text_review\", \"label\"] if c in df.columns])\n",
    "        )\n",
    "\n",
    "    return category_dfs\n",
    "    \n",
    "train_category_dfs = trim_review_to_category_relevant_text(df_train_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56d9357c-8586-4427-bf6e-d4f5f6ccb42a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(train_category_dfs['comfort'].limit(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59e54c46-9b14-4b9d-a082-47e0ef0dbc5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Keep only reviews with at least 10 characters to drop very short texts.\n",
    "train_category_dfs = {\n",
    "    ctg: df_ctg.filter(F.length(F.trim(F.col(\"text_review\"))) >= 10)\n",
    "    for ctg, df_ctg in train_category_dfs.items()\n",
    "}\n",
    "\n",
    "for ctg, df in train_category_dfs.items():\n",
    "    print(f\"{ctg}: {df.count()}\")\n",
    "\n",
    "display(train_category_dfs['comfort'].limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14556002-52a2-4ab4-a830-c49f235aa606",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Find each review its sentiment:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6538b3e8-0adc-41ed-8426-df661f54d0e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_category_dfs = {\n",
    "    ctg: add_sentiment_to_review(df_ctg, text_col=\"text_review\")\n",
    "    for ctg, df_ctg in train_category_dfs.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79bb34bc-1a47-408a-a9c0-a256bdbb68d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(train_category_dfs['comfort'].limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fa2c4fc-ccd4-4f49-8711-f2f125405f3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Update Review Score From Category Review Semantics:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aac76287-f233-490e-98fd-b4cac1584187",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This block creates an adjusted score based on the sentiment of the category-specific text.\n",
    "# First it detects sentiment (positive/negative/neutral), then marks “hard” sentiment using strong words.\n",
    "# Finally it updates the score: hard sentiment changes it by ±4, regular sentiment changes it by ±2, and clamps the result to 1–10.\n",
    "\n",
    "hard_neg_re = r\"(terrible|awful|horrible|disgusting|unacceptable|worst|broken|ridiculous|appalling)\"\n",
    "hard_pos_re = r\"(amazing|excellent|perfect|outstanding|fantastic|wonderful|exceptional|incredible)\"\n",
    "\n",
    "\n",
    "def adjust_review_score(df):\n",
    "    df_scored = (\n",
    "        df\n",
    "        .withColumn(\"is_hard_neg\", (F.col(\"sentiment\") == \"negative\") & F.lower(F.col(\"text_review\")).rlike(hard_neg_re))\n",
    "        .withColumn(\"is_hard_pos\", (F.col(\"sentiment\") == \"positive\") & F.lower(F.col(\"text_review\")).rlike(hard_pos_re))\n",
    "\n",
    "        # apply scoring rules (hard first, then regular)\n",
    "        .withColumn(\n",
    "            \"rating_adj_raw\",\n",
    "            F.when(F.col(\"is_hard_neg\") & (F.col(\"label\") >= 6), F.col(\"label\") - F.lit(4.0))\n",
    "            .when(F.col(\"is_hard_pos\") & (F.col(\"label\") <= 6), F.col(\"label\") + F.lit(4.0))\n",
    "            .when((F.col(\"sentiment\") == \"negative\") & (F.col(\"label\") >= 6), F.col(\"label\") - F.lit(2.0))\n",
    "            .when((F.col(\"sentiment\") == \"positive\") & (F.col(\"label\") <= 6), F.col(\"label\") + F.lit(2.0))\n",
    "            .otherwise(F.col(\"label\"))\n",
    "        )\n",
    "        # clamp to valid range\n",
    "        .withColumn(\"label\", F.least(F.lit(10.0), F.greatest(F.lit(1.0), F.col(\"rating_adj_raw\"))))\n",
    "        .drop(\"rating_adj_raw\")\n",
    "    )\n",
    "    return df_scored.select(\"text_review\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b444be28-e4db-425c-bf54-121a6993447d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_dfs = {\n",
    "    ctg: adjust_review_score(df_ctg)\n",
    "    for ctg, df_ctg in train_category_dfs.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91f52ebe-5983-422d-bc68-995d935c71cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(train_dfs['comfort'].limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e1e9cb8-a87c-4594-bf82-9c90016a07d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**for EDA:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a860b304-a74f-4439-bff5-31358edf40f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_df_train = df_train.sample(withReplacement=False, fraction=0.2, seed=42)\n",
    "sample_df_train = create_categories_column(sample_df_train, \"text_review\")\n",
    "sample_df_train_long = sample_df_train.select(\n",
    "        \"hotel_id\",\n",
    "        \"text_review\",\n",
    "        \"label\",\n",
    "        F.explode(\"categories\").alias(\"category\")\n",
    "    )\n",
    "sample_train_category_dfs = trim_review_to_category_relevant_text(sample_df_train_long)\n",
    "sample_train_category_dfs = {\n",
    "    ctg: add_sentiment_to_review(df_ctg, text_col=\"text_review\")\n",
    "    for ctg, df_ctg in sample_train_category_dfs.items()\n",
    "}\n",
    "sample_train_category_dfs = {\n",
    "    ctg: adjust_review_score(df_ctg)\n",
    "    for ctg, df_ctg in sample_train_category_dfs.items()\n",
    "}\n",
    "\n",
    "display(sample_train_category_dfs[\"comfort\"].limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bb52387-4f9d-4da1-b971-2d7832689a13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "avg_dfs = []\n",
    "for ctg, df_ctg in sample_train_category_dfs.items():\n",
    "    avg_dfs.append(\n",
    "        df_ctg\n",
    "        .select(\"label\")\n",
    "        .withColumn(\"category\", F.lit(ctg))\n",
    "        .withColumn(\"stage\", F.lit(\"before\"))\n",
    "        .groupBy(\"stage\", \"category\")\n",
    "        .agg(F.avg(\"label\").alias(\"avg_label\"))\n",
    "    )\n",
    "\n",
    "avg_by_category = reduce(lambda a, b: a.unionByName(b), avg_dfs)\n",
    "pdf = avg_by_category.orderBy(\"category\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b695e81b-d68b-4916-92f0-3eb87aaf4c00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cats = pdf[\"category\"].tolist()\n",
    "vals = pdf[\"avg_label\"].tolist()\n",
    "x = list(range(len(cats)))\n",
    "\n",
    "plt.scatter(x, vals, s=70)\n",
    "\n",
    "plt.xticks(x, cats, fontsize=10)\n",
    "plt.margins(x=0.03)\n",
    "plt.xlim(-0.5, len(x)-1 + 0.5)\n",
    "\n",
    "plt.ylim(7, 10)\n",
    "\n",
    "for xi, yi in zip(x, vals):\n",
    "    plt.text(\n",
    "        xi, yi + 0.08,\n",
    "        f\"{yi:.2f}\",\n",
    "        ha=\"center\", va=\"bottom\", fontsize=9\n",
    "    )\n",
    "\n",
    "plt.ylabel(\"average rating\")\n",
    "plt.title(\"Average Rating per Category (train)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97999d09-3c32-4439-af36-519cf4a42049",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**______________________________**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f55991f8-17fa-46ba-a7f4-978cb0448e93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Linear Regression:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07d2b9d4-f3dc-4051-a31d-99fd26b86077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_regression_linear_model(train_df):\n",
    "    \"\"\"\n",
    "    Trains a text-based linear regression model using Word2Vec embeddings.\n",
    "    The function tokenizes review text, removes stop words, learns word embeddings,\n",
    "    and fits a regularized linear regression to predict numeric review scores.\n",
    "\n",
    "    train_df:\n",
    "        text_review : string\n",
    "        label       : double\n",
    "    \"\"\"\n",
    "    tokenizer = RegexTokenizer(\n",
    "        inputCol=\"text_review\",\n",
    "        outputCol=\"tokens\",\n",
    "        pattern=\"\\\\W+\",\n",
    "        minTokenLength=2\n",
    "    )\n",
    "\n",
    "    remover = StopWordsRemover(\n",
    "        inputCol=\"tokens\",\n",
    "        outputCol=\"filtered_tokens\"\n",
    "    )\n",
    "\n",
    "    w2v = Word2Vec(\n",
    "        inputCol=\"filtered_tokens\",\n",
    "        outputCol=\"features\",\n",
    "        vectorSize=50,\n",
    "        windowSize=3,\n",
    "        minCount=2\n",
    "    )\n",
    "\n",
    "    lr = LinearRegression(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"label\",\n",
    "        predictionCol=\"prediction\",\n",
    "        regParam=0.05,          # regularization קל לריאליזם\n",
    "        elasticNetParam=0.0    # Ridge-style\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(stages=[tokenizer, remover, w2v, lr])\n",
    "    model = pipeline.fit(train_df)\n",
    "    return model\n",
    "\n",
    "\n",
    "def estimate_noise_sigma(model, train_df):\n",
    "    \"\"\"\n",
    "    Estimates the standard deviation of the model's prediction error (noise)\n",
    "        by computing residuals (label - prediction) on the training data.\n",
    "    This value is later used to add realistic Gaussian noise to predictions.\n",
    "    \"\"\"\n",
    "    preds = model.transform(train_df)\n",
    "    preds = preds.withColumn(\"residual\", F.col(\"label\") - F.col(\"prediction\"))\n",
    "    sigma = (\n",
    "        preds\n",
    "        .agg(F.stddev(\"residual\").alias(\"sigma\"))\n",
    "        .collect()[0][\"sigma\"]\n",
    "    )\n",
    "    return float(sigma) if sigma is not None else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbcb47b8-35f6-4ed2-a432-37bbcfaccf57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train a separate linear regression model (with noise estimation) for each category\n",
    "\n",
    "models = {}\n",
    "for ctg, train_df in train_dfs.items():\n",
    "    model = train_regression_linear_model(train_df)\n",
    "    sigma = estimate_noise_sigma(model, train_df)\n",
    "    models[ctg] = (model, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc1cf390-9736-4e75-a688-9be18a6c0073",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Predictions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "507892d0-7ee5-4ca3-9bef-7d1191525c12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def predict_with_regression_linear_model(model, df, sigma, noise_scale=0.5):\n",
    "    \"\"\"\n",
    "    Generates predictions using a trained model and adds Gaussian noise\n",
    "    to simulate realistic variability in text-based score predictions.\n",
    "\n",
    "    noise_scale:\n",
    "        1.0 = realistic noise level\n",
    "        <1  = reduced noise\n",
    "        >1  = increased noise\n",
    "    \"\"\"\n",
    "    preds = model.transform(df)\n",
    "    preds = preds.withColumn(\n",
    "        \"prediction\",\n",
    "        F.col(\"prediction\") + F.lit(sigma * noise_scale) * F.randn()\n",
    "    )\n",
    "    preds = preds.withColumn(\n",
    "        \"prediction\",\n",
    "        F.greatest(F.lit(1.0), F.least(F.col(\"prediction\"), F.lit(10.0)))\n",
    "    )\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fed13f4f-462c-452d-8092-8ccf8d2b973f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TOO_SMALL_REVIEWS_NUM = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b28a40ac-9137-403c-9a87-4d7a850925b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# For each hotel, assign reviews to categories, predict noisy scores with a category-specific regression model,\n",
    "# and aggregate per (hotel_id, category) to produce final category scores, review counts, and representative examples.\n",
    "\n",
    "df_test = create_categories_column(df_test, \"text_review\")\n",
    "test_long = df_test.select(\n",
    "    \"hotel_id\",\n",
    "    \"text_review\",\n",
    "    F.explode(\"categories\").alias(\"category\")\n",
    ")\n",
    "test_category_dfs = trim_review_to_category_relevant_text(test_long)\n",
    "\n",
    "K = 3   # number of example reviews to keep per category\n",
    "categories_scores = []\n",
    "\n",
    "for ctg, (model, sigma) in models.items():\n",
    "    w_cnt = Window.partitionBy(\"hotel_id\")\n",
    "\n",
    "    df_ctg = (\n",
    "        test_category_dfs[ctg]\n",
    "        .select(\"hotel_id\", \"text_review\")\n",
    "        .withColumn(\"number_reviews\", F.count(\"*\").over(w_cnt))\n",
    "    )\n",
    "\n",
    "    # Save hotel's category information only if there are enough reviews\n",
    "    df_enough_reviews = df_ctg.filter(F.col(\"number_reviews\") >= TOO_SMALL_REVIEWS_NUM)\n",
    "\n",
    "    # If no hotel has enough reviews in this category, move to next one\n",
    "    if df_enough_reviews.rdd.isEmpty():\n",
    "        continue\n",
    "\n",
    "    preds_ctg = predict_with_regression_linear_model(model, df_enough_reviews, sigma)\n",
    "\n",
    "    avg_preds = preds_ctg.groupBy(\"hotel_id\").agg(\n",
    "        F.lit(ctg).alias(\"category\"),\n",
    "        F.max(\"number_reviews\").alias(\"number_reviews\"),\n",
    "        F.round(F.avg(\"prediction\"), 3).alias(\"score\")\n",
    "    )\n",
    "\n",
    "    # save K reviews per hotel to \"show\" why they got their score \n",
    "    w_mean = Window.partitionBy(\"hotel_id\")\n",
    "    w_rank = Window.partitionBy(\"hotel_id\").orderBy(F.col(\"abs_diff\").asc())\n",
    "\n",
    "    examples_df = (\n",
    "        preds_ctg\n",
    "        .withColumn(\"mean\", F.avg(\"prediction\").over(w_mean))\n",
    "        .withColumn(\"abs_diff\", F.abs(F.col(\"prediction\") - F.col(\"mean\")))\n",
    "        .withColumn(\"rn\", F.row_number().over(w_rank))\n",
    "        .filter(F.col(\"rn\") <= K)\n",
    "        .groupBy(\"hotel_id\")\n",
    "        .agg(F.collect_list(F.col(\"text_review\")).alias(\"example_reviews\"))\n",
    "    )\n",
    "\n",
    "    avg_preds = (\n",
    "        avg_preds\n",
    "        .join(examples_df, on=\"hotel_id\", how=\"inner\")\n",
    "        .withColumn(\"example_reviews\", F.coalesce(\"example_reviews\", F.array()))\n",
    "    )\n",
    "\n",
    "    categories_scores.append(avg_preds)\n",
    "\n",
    "# Union all categories into one table: many rows (hotel_id, category)\n",
    "category_summary = reduce(lambda a, b: a.unionByName(b), categories_scores)\n",
    "\n",
    "category_summary = category_summary.select(\n",
    "    \"hotel_id\", \"category\", \"score\", \"example_reviews\", \"number_reviews\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86932fb0-5bba-4350-a792-f372052667a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(category_summary.limit(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b4018db-988d-4ee0-81d7-e3f9a42536f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hotels_id_with_real_category_scores = scraped_booking_real_scores.select(\"hotel_id\").distinct()\n",
    "hotels_summary_for_tool = hotels_id_with_real_category_scores.join(category_summary, on=\"hotel_id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "faa4903b-05aa-4793-9bb1-c5f463639849",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**tool input:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f59140be-1ec3-4e6f-9e5f-f407e18d409d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hotels_summary_as_json = (\n",
    "    hotels_summary_for_tool\n",
    "    .groupBy(\"hotel_id\")\n",
    "    .agg(\n",
    "        F.to_json(\n",
    "            F.map_from_entries(\n",
    "                F.collect_list(\n",
    "                    F.struct(\n",
    "                        F.col(\"category\"),\n",
    "                        F.struct(\n",
    "                            F.col(\"score\").alias(\"score\"),\n",
    "                            F.col(\"number_reviews\").alias(\"number_reviews\"),\n",
    "                            F.col(\"example_reviews\").alias(\"examples\"),\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        ).alias(\"hotel_categories_score\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"needs to be <= 120, row count is\", hotels_summary_as_json.count())\n",
    "display(hotels_summary_as_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51dd236e-d03e-4ff2-bb66-abb75153f73a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"unique hotel_id:\", category_summary.select(\"hotel_id\").distinct().count())\n",
    "print(\"Rows per category:\")\n",
    "category_summary.groupBy(\"category\").count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e3c9c5b-3802-44b3-ac0a-95b48bbb28cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**for EDA:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98889556-5543-44e4-8d92-4d84c3e8afd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Graph AVG real and pred category scores for same hotels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dae434f-0c98-4bea-9b8e-2b78630fd7d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ctg_cols = [\"staff\", \"facilities\", \"cleanliness\", \"comfort\", \"location\", \"free_wifi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c37e1af-e319-4bfd-9d7f-81d51bb5bb68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_avg_real_long = (\n",
    "    scraped_booking_real_scores\n",
    "    .select(F.explode(F.array(*[\n",
    "        F.struct(\n",
    "            F.lit(c).alias(\"category\"),\n",
    "            F.col(c).cast(\"double\").alias(\"score\")\n",
    "        )\n",
    "        for c in ctg_cols\n",
    "    ])).alias(\"x\"))\n",
    "    .select(\"x.category\", \"x.score\")\n",
    "    .filter(F.col(\"score\").isNotNull() & (F.col(\"score\") != 0))\n",
    "    .groupBy(\"category\")\n",
    "    .agg(F.avg(\"score\").alias(\"avg\"))\n",
    "    .withColumn(\"source\", F.lit(\"real\"))\n",
    ")\n",
    "display(df_avg_real_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2079884d-5ca5-4f4d-868a-037098615a68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_avg_pred_long = (\n",
    "    hotels_summary_for_tool\n",
    "    .filter(F.col(\"category\").isin(ctg_cols))\n",
    "    .filter(F.col(\"score\").isNotNull() & (F.col(\"score\") != 0))\n",
    "    .groupBy(\"category\")\n",
    "    .agg(F.avg(\"score\").alias(\"avg\"))\n",
    "    .withColumn(\"source\", F.lit(\"pred\"))\n",
    ")\n",
    "display(df_avg_pred_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9764e615-c5b4-4903-81f0-99ae7bfe6770",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_compare = df_avg_real_long.unionByName(df_avg_pred_long)\n",
    "display(df_compare)\n",
    "pdf = df_compare.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42dd3b57-b990-45ca-954b-8dd40f0c064b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cats = sorted(pdf[\"category\"].unique())\n",
    "x_map = {c: i for i, c in enumerate(cats)}\n",
    "pdf[\"x\"] = pdf[\"category\"].map(x_map)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "real = pdf[pdf.source == \"real\"]\n",
    "pred = pdf[pdf.source == \"pred\"]\n",
    "\n",
    "plt.scatter(real[\"x\"], real[\"avg\"], marker=\"o\", s=80, label=\"avg real\")\n",
    "plt.scatter(pred[\"x\"], pred[\"avg\"], marker=\"^\", s=80, label=\"avg ours\")\n",
    "\n",
    "# add score labels (2 digits after decimal) slightly above each point\n",
    "for _, r in real.iterrows():\n",
    "    plt.text(r[\"x\"], r[\"avg\"] + 0.03, f'{r[\"avg\"]:.2f}', ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "for _, r in pred.iterrows():\n",
    "    plt.text(r[\"x\"], r[\"avg\"] + 0.03, f'{r[\"avg\"]:.2f}', ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "plt.xticks(range(len(cats)), cats)\n",
    "plt.ylabel(\"average\")\n",
    "plt.title(\"Average Rating per Category: real vs ours\")\n",
    "plt.legend()\n",
    "plt.ylim(7, 10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63d638cf-cb8f-4430-a953-72f7e3feb4fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cats = sorted(pdf[\"category\"].unique())\n",
    "x_map = {c: i for i, c in enumerate(cats)}\n",
    "pdf[\"x\"] = pdf[\"category\"].map(x_map)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "real = pdf[pdf.source == \"real\"]\n",
    "pred = pdf[pdf.source == \"pred\"]\n",
    "\n",
    "plt.scatter(real[\"x\"], real[\"avg\"], marker=\"o\", s=80, label=\"avg real\")\n",
    "plt.scatter(pred[\"x\"], pred[\"avg\"], marker=\"^\", s=80, label=\"avg ours\")\n",
    "\n",
    "plt.xticks(range(len(cats)), cats)\n",
    "plt.ylabel(\"average\")\n",
    "plt.title(\"Average Rating per Category: real vs ours\")\n",
    "plt.legend()\n",
    "plt.ylim(7, 10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47cdd20e-0aaf-44db-b9f8-05e4808b8b3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**examples for predictions for each category:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68f31a7a-48dc-4c62-b720-cfd75db08570",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sample_test = df_test.sample(withReplacement=False, fraction=0.0003, seed=42).cache()\n",
    "print(\"number of rows is\", df_sample_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76cc2c42-5af4-4b7a-a940-ff80dd49b5f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sample_test = create_categories_column(df_sample_test, \"text_review\")\n",
    "sample_test_long = df_sample_test.select(\n",
    "    \"hotel_id\",\n",
    "    \"text_review\",\n",
    "    F.explode(\"categories\").alias(\"category\")\n",
    ")\n",
    "sample_test_category_dfs = trim_review_to_category_relevant_text(sample_test_long)\n",
    "\n",
    "for ctg, (model, sigma) in models.items():\n",
    "    w_cnt = Window.partitionBy(\"hotel_id\")\n",
    "\n",
    "    df_sample_ctg = (\n",
    "        sample_test_category_dfs[ctg]\n",
    "        .select(\"hotel_id\", \"text_review\")\n",
    "    )\n",
    "\n",
    "    preds_ctg = predict_with_regression_linear_model(model, df_sample_ctg, sigma).select(\"hotel_id\", \"text_review\", \"prediction\")\n",
    "    print(\"example for prediction for category \", ctg, \":\")\n",
    "    display(preds_ctg.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b3564a2-56aa-4751-85b6-32103d9497f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**examples for category avg for tool for each category:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31ad3a33-b01f-4847-9dc4-7234c7532558",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "w = Window.partitionBy(\"category\").orderBy(F.rand())\n",
    "\n",
    "category_summary_10 = (\n",
    "    category_summary\n",
    "    .withColumn(\"rn\", F.row_number().over(w))\n",
    "    .filter(F.col(\"rn\") <= 10)\n",
    "    .drop(\"rn\")\n",
    ")\n",
    "\n",
    "display(category_summary_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbde1ba4-712c-4c6d-b591-19b4302700ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) maabada1 project",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}